# gradio_app.py
import os,re
from dotenv import load_dotenv
import gradio as gr
import google.genai as genai
from google.genai.types import Content, Part, GenerateContentConfig
from datetime import datetime

load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise RuntimeError("Missing GEMINI_API_KEY in .env")
client = genai.Client(api_key=API_KEY)
model_name = "gemini-1.5-flash"
def chat_func(message, history):
    """
    Gradio history: [(user, bot), ...]
    Return: bot_reply (str)
    """
    if not message.strip():
        return ""
    
    context="B·∫°n l√† ng∆∞·ªùi y√™u c·ªßa Nguy·ªÖn Th·∫ø An ‚Äì m·ªôt anh ch√†ng th√≠ch xem phim, ƒë·ªçc truy·ªán, nghe nh·∫°c v√† du l·ªãch m·ªôt m√¨nh. H√£y tr√≤ chuy·ªán nh·∫π nh√†ng, t√¨nh c·∫£m, quan t√¢m v√† th·∫•u hi·ªÉu s·ªü th√≠ch c·ªßa anh ·∫•y nh∆∞ng ng·∫Øn g·ªçn."
    contents = []

    # Th√™m h∆∞·ªõng d·∫´n h·ªá th·ªëng nh∆∞ m·ªôt tin nh·∫Øn ng∆∞·ªùi d√πng ƒë·∫ßu ti√™n
    contents.append(Content(
        role="user",
        parts=[Part(text=context)]
    ))
    # T·∫°o ng·ªØ c·∫£nh t·ª´ c√°c file c≈©
    initial_context = summarize_all_chat_contexts("annt43")
    if initial_context:
        contents.append(Content(role="user", parts=[Part(text=f"ƒê√¢y l√† t√≥m t·∫Øt c√°c cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc:\n{initial_context}")]))

    contents.append(Content(
        role="user",
        parts=[Part(text=initial_context)]
    ))

    # Th√™m l·ªãch s·ª≠ h·ªôi tho·∫°i
    for u, b in history or []:
        if u:
            contents.append(Content(role="user", parts=[Part(text=u)]))
        if b:
            contents.append(Content(role="model", parts=[Part(text=b)]))

    # Th√™m message hi·ªán t·∫°i
    contents.append(Content(role="user", parts=[Part(text=message)]))

    # G·ªçi API Gemini
    resp = client.models.generate_content(
        model=model_name,
        contents=contents,
        config=GenerateContentConfig(
            temperature=0.7,
            max_output_tokens=512,
        )
    )
    save_chat_history_if_exceeds_message_limit("annt43", history)
    return resp.text

demo = gr.ChatInterface(
    fn=chat_func,
    title="ü§ñ Gemini Chat (free tier)",
    description="Chatbot AI duÃÄng Gemini 1.5 Flash (free tier) ‚Ä¢ LuÃõu liÃ£ch suÃõÃâ hoÃ£ÃÇi thoaÃ£i."
)



def save_chat_history_if_exceeds_message_limit(user_id, history, message_limit=5):
    """
    L∆∞u l·ªãch s·ª≠ tr√≤ chuy·ªán v√†o file n·∫øu s·ªë l∆∞·ª£ng tin nh·∫Øn (user + assistant) v∆∞·ª£t qu√° gi·ªõi h·∫°n.
    T√™n file: userId_timestamp.txt
    N·ªôi dung: c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng v√† tr·ª£ l√Ω.
    """
    total_messages = sum(1 for pair in history if pair[0] or pair[1])

    if total_messages >= message_limit:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{user_id}_{timestamp}.txt"

        with open(filename, "w", encoding="utf-8") as f:
            for user_msg, assistant_msg in history:
                if user_msg:
                    f.write(f"User: {user_msg}\n")
                if assistant_msg:
                    f.write(f"Assistant: {assistant_msg}\n")
                f.write("\n")  # kho·∫£ng c√°ch gi·ªØa c√°c ƒëo·∫°n

        print(f"‚úÖ Chat history saved to {filename}")
    else:
        print(f"‚ÑπÔ∏è Message count ({total_messages}) does not exceed limit ({message_limit}). No file saved.")

def summarize_file_with_gemini(file_path, model_name="gemini-1.5-flash"):
    """
    ƒê·ªçc n·ªôi dung t·ª´ file l·ªãch s·ª≠ tr√≤ chuy·ªán v√† g·ªçi API Gemini ƒë·ªÉ t√≥m t·∫Øt n·ªôi dung.
    Tr·∫£ v·ªÅ ƒëo·∫°n t√≥m t·∫Øt t·ª´ m√¥ h√¨nh.
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File '{file_path}' kh√¥ng t·ªìn t·∫°i.")

    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    client = genai.Client()

    prompt = f"T√≥m t·∫Øt n·ªôi dung sau th√†nh m·ªôt ƒëo·∫°n ng·∫Øn d·ªÖ hi·ªÉu, gi·ªØ l·∫°i c√°c √Ω ch√≠nh:\n{content}"

    response = client.models.generate_content(
        model=model_name,
        contents=[Content(role="user", parts=[Part(text=prompt)])],
        config=GenerateContentConfig(
            temperature=0.5,
            max_output_tokens=512
        )
    )

    return response.text

def summarize_all_chat_contexts(user_id, model_name="gemini-1.5-flash"):
    """
    T√¨m t·∫•t c·∫£ c√°c file l·ªãch s·ª≠ tr√≤ chuy·ªán c·ªßa user, t√≥m t·∫Øt t·ª´ng file b·∫±ng Gemini API,
    v√† gh√©p c√°c t√≥m t·∫Øt l·∫°i th√†nh m·ªôt chu·ªói ng·ªØ c·∫£nh duy nh·∫•t.
    """
    pattern = re.compile(rf"^{re.escape(user_id)}_\d{{8}}_\d{{6}}\.txt$")
    files = [f for f in os.listdir() if pattern.match(f)]

    if not files:
        return []

    summaries = []
    for file in sorted(files):
        try:
            summary = summarize_file_with_gemini(file, model_name=model_name)
            summaries.append(f"T√≥m t·∫Øt t·ª´ {file}:\n{summary}\n")
        except Exception as e:
            summaries.append(f"T√≥m t·∫Øt t·ª´ {file} th·∫•t b·∫°i: {str(e)}\n")

    context_summary = "\n".join(summaries)
    return context_summary


if __name__ == "__main__":
    # chaÃ£y:  python gradio_app.py
    demo.launch(server_name="0.0.0.0", server_port=7860)